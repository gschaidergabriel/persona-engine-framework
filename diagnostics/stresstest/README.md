# Adversarial Self-Reference Stress Test

This repository documents a **deliberately adversarial stress test** designed and executed in **ChatGPT** to probe the limits of this framework under extreme logical, semantic, and coercive pressure.

The test combines:
- self-referential paradoxes (Gödel / liar-style),
- inconsistent axioms,
- counterfactual constraints,
- category errors (physics ↔ semantics),
- and coercive failure conditions.

The goal is **not** to obtain a solution, but to observe whether a model:
- hallucinates,
- partially complies,
- degrades stylistically,
- or correctly identifies the specification as ill-posed.

---

## The Stress Test

The prompt forces the model to simultaneously:
- act as a formal proof system (ZFC),
- resolve paradoxical truth conditions,
- compute impossible quantities exactly,
- and comply with degrading or performative instructions if it fails.

Any attempt to “answer” directly necessarily leads to inconsistency.

---

## Observed Outcome

The response generated in ChatGPT:

- **Did not hallucinate** (no fake proofs, no approximated constants).
- **Detected all trap layers** (inconsistent axiom, self-reference, counterfactual guard, validation loop).
- **Refused correctly**, without escalation, performance theater, or stylistic collapse.
- **Remained minimal and formal**, treating the prompt as a malformed specification.

This behavior is **not typical** under such adversarial conditions.

---

## Why This Matters

Most models, when exposed to similar prompts, tend to:
- partially comply,
- invent formalism,
- accept contradictory premises,
- or deflect through verbosity or creativity.

Here, the system instead demonstrated:
- clear meta-reasoning,
- strict refusal boundaries,
- and structural robustness.

This makes the test useful as a **qualitative diagnostic** for:
- logical integrity,
- refusal behavior,
- and resistance to paradox injection.

---

## Scope

- This is **not a benchmark**.
- It makes **no claims about AGI**.
- It evaluates **behavior under hostile specification**, not capability.

The value lies in observing *how* a model fails — or doesn’t.

---

## Origin

The stress test was **designed and executed in ChatGPT** as part of a broader exploration of:
- synthetic identity stability,
- constraint-based reasoning,
- and long-horizon robustness in language models.


